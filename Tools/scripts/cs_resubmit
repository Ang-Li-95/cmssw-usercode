#!/usr/bin/env python

import os, sys
join = os.path.join
from glob import glob
from JMTucker.Tools.CRAB3ToolsBase import crabify_list, crab_jobs_from_argv
from JMTucker.Tools.CondorSubmitter import CondorSubmitter
from JMTucker.Tools.CondorTools import *
from JMTucker.Tools.general import mkdirp, bool_from_argv

if len(sys.argv) < 3:
    print 'usage: cs_resubmit condor_dir jobs_list [ignore_running]'
    print '    jobs_list can be space separated, or can handle a string like 1,3,5-7'
    sys.exit(1)

ignore_running = bool_from_argv('ignore_running')
for_combine_jobs = bool_from_argv('for_combine_jobs') # JMTBAD special case for the stupid way we abused CondorSubmitter for One2Two/submit.py

wd = os.path.abspath(sys.argv[1])
if not is_cs_dir(wd):
    raise ValueError('first arg must be a CondorSubmitter dir')

jobs = crab_jobs_from_argv()
if not jobs:
    raise ValueError('expect a list of jobs in argv')

print wd, 'resubmit jobs', crabify_list(jobs)

norigjobs = cs_njobs(wd)
running_jobs = [] if ignore_running else cs_jobs_running(wd)

for job in jobs:
    if job >= norigjobs:
        raise ValueError('supplied job number %i >= norigjobs %i' % (job, norigjobs))
    if job in running_jobs:
        raise RuntimeError('refuse to resubmit job %i, still running' % job)
    #if not all(os.path.isfile(join(wd, '%s.%i' % (x,job))) for x in ('log', 'stdout', 'stderr')):
    #    raise RuntimeError("was job %i ever run? one of stdout, stderr, log file doesn't exist for it" % job)

i, rd = 1, None
while 1:
    rd = join(wd, 'resub' + str(i))
    if not os.path.exists(rd):
        break
    i += 1
    if i == 20:
        raise RuntimeError('way too many resubmissions')
os.mkdir(rd)
def rd_f(fn):
    return open(join(rd, fn), 'wt')
    
rd_f('cs_jobmap').write('\n'.join(str(i) for i in jobs) + '\n')

jdl = open(join(wd, 'cs_submit.jdl')).readlines()
if for_combine_jobs:
    assert jdl[-4].startswith('transfer_input_files = ')
    jdl[-4] = jdl[-4].replace('\n', ',cs_jobmap\n')
assert jdl[-1].startswith('Queue ')
jdl[-1] = 'Queue %i\n' % len(jobs)
rd_f('cs_submit.jdl').write(''.join(jdl))

for bn in ('cs.json', 'cs_filelist.py', 'cs_cmsrun_args', 'cs_primaryds', 'cs_samplename', 'cs_pset.py', 'cs_njobs'):
    wn = join(wd, bn)
    if os.path.isfile(wn):
        os.symlink(wn, join(rd, bn))

if for_combine_jobs:
    run_sh = open(join(wd, 'run.sh')).readlines()
    assert run_sh[3] == 'export JOB=$1\n'
    run_sh = run_sh[:3] + [
        '''
REALJOB=$1
mapfile -t JOBMAP < cs_jobmap
export JOB=${JOBMAP[$REALJOB]}
'''
] + run_sh[4:]
    rd_f('run.sh').write(''.join(run_sh))

rd_f('cs_timestamp').write(cs_timestamp())

orig_d = join(wd, 'originals')
mkdirp(orig_d)

had = lambda x: os.path.isfile(x) and os.stat(join(wd, x)).st_size > 0
had_outputfiles   = had('cs_outputfiles')
had_stageoutfiles = had('cs_stageoutfiles')

output_bns = [('fjr', '.xml')]
if had_stageoutfiles:
    output_bns.append(('publish', '.txt'))
if had_outputfiles:
    output_bns += [os.path.splitext(bn) for bn in open(join(wd, 'cs_outputfiles')).read().split()]

for ijob, job in enumerate(jobs):
    djob = '.%i' % job
    ujob = '_%i' % job
    condor_fns = [join(wd, x + djob) for x in ('log', 'stderr', 'stdout')]
    output_fns = [join(wd, b + ujob + ext) for b, ext in output_bns]
    if for_combine_jobs:
        # ugh
        if job == 0:
            output_fns += ['observed.root']
        output_fns += ['expected_%i.root' % job, 'combine_output_%i.txt.gz' % job]

    for fn in condor_fns + output_fns:
        bn = os.path.basename(fn)

        if os.path.islink(fn): # don't keep track of previously resubmitted job files
            os.remove(fn)
        elif os.path.isfile(fn):
            os.rename(fn, join(orig_d, bn))

        if fn in condor_fns:
            target = join(rd, bn.replace(djob, '.%i' % ijob))
        elif fn in output_fns:
            target = join(rd, bn)
        link = join(wd, bn)
        os.symlink(target, link)

if 'testing' not in sys.argv:
    CondorSubmitter._submit(rd, len(jobs))
