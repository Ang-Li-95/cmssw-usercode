#!/usr/bin/env python

# author: J. Tucker

import datetime
import optparse
import os
import pdb
import sys
import time
from collections import defaultdict
from JMTucker.Tools.CRAB3Tools import *
from JMTucker.Tools import colors

parser = optparse.OptionParser(usage='%prog [options] <space-separated list of crab dirs>')
parser.add_option('-v', '--verbose', action='store_true', default=False,
                  help='Print way more.')
parser.add_option('-c', '--continuous', action='store_true', default=False,
                  help='Keep making passes.')
parser.add_option('-w', '--wait-time', type='int', default=-1,
                  help='Time between passes, in seconds. Default is %default.')
parser.add_option('-d', '--delay', action='store_true', default=False,
                  help='Wait once before starting passes.')
parser.add_option('--max-processes', type='int', default=5,
                  help='Max number of status-checking processes to run at once.')
parser.add_option('-r', '--resub', action='store_true', default=False,
                  help='Send resubmit request to any batch that has failed jobs, obeying the white and black lists below.')
#parser.add_option('--resub-white-list', default='',
#                  help='Comma-separated list (no spaces!) of sites to whitelist when resubmitting.')
#parser.add_option('--resub-black-list', default='',
#                  help='Comma-separated list (no spaces!) of sites to blacklist when resubmitting.')
#parser.add_option('--resub-white-codes', default='',
#                  help='Comma-separated list (no spaces!) of codes that are allowed to resubmit -- all others not resubmitted.')
#parser.add_option('--resub-black-codes', default='',
#                  help='Comma-separated list (no spaces!) of codes that are not allowed to resubmit -- all others are resubmitted.')
parser.add_option('--fake-results', type='str', metavar='FILE',
                  help='Read fake results dict from FILE.')
parser.add_option('--no-dir-sort', action='store_false', dest='dir_sort', default=True,
                  help='Do not resort the list of directories.')
parser.add_option('-n', '--no-renew-proxies', action='store_false', dest='renew_proxies', default=True,
                  help='Do not renew proxies first.')
parser.add_option('--no-shorten-statuses', action='store_false', dest='shorten_statuses', default=True,
                  help='Do not shorten statuses in the status table.')
parser.add_option('--no-suppress-zeroes', action='store_false', dest='suppress_zeroes', default=True,
                  help='Do not suppress zeroes in the status table.')
parser.add_option('--post-process', type='str', metavar='FILE.PY',
                  help='Call the function "post_process_fcn" defined in FILE.PY, which takes the directory name and results dict as its arguments.')
options, args = parser.parse_args()
#print options ; print args ; import sys ; print sys.argv ; raise 1

################################################################################

def print_header(s):
    print '\033[1m%s\033[0m' % s
def print_subheader(s):
    print '\033[32m%s\033[0m' % s

#if options.resub_white_list and options.resub_black_list:
#    raise ValueError('cannot have both --resub-white-list and --resub-black-list')
#
#if options.resub_white_codes and options.resub_black_codes:
#    raise ValueError('cannot have both --resub-white-codes and --resub-black-codes')
#
#options.resub_white_codes = options.resub_white_codes.split(',')
#options.resub_black_codes = options.resub_black_codes.split(',')

if options.continuous and options.fake_results:
    print '--fake-results implies not continuous, setting it.'
    options.continuous = False

if options.wait_time > 0 or options.delay:
    options.continuous = True
if options.continuous and options.wait_time < 0:
    options.wait_time = 60*10

if options.post_process:
    raise NotImplementedError('--post-process not implemented')
    #exec open(options.post_process).read()
    if options.post_process.endswith('.py'):
        options.post_process = options.post_process.replace('.py', '')
    pp_dir, pp_fn = os.path.split(options.post_process)
    sys.path.append(os.path.abspath(pp_dir))
    try:
        post_process_module = __import__(pp_fn)
    except ImportError:
        raise ImportError('--post-process: cannot find module %s' % options.post_process)
    try:
        post_process_fcn = post_process_module.post_process_fcn
    except AttributeError:
        raise AttributeError('--post-process: module %s does not define a "post_process_fcn" function' % options.post_process)
else:
    post_process_fcn = None

def timestamp():
    return str(int(time.time()*1e6))

backdoor_time = timestamp()

def backdoor_file(filename):
    fn = '/tmp/%s/crmon_%s_%s' % (os.environ['USER'], backdoor_time, filename)
    return fn, open(fn, 'wt')

def backdoor_print(filename, l):
    ll = []
    for a in l:
        if a.endswith('\\'):
            a = a.rsplit('\\')[0]
        ll.append(a)
    s = '\n'.join(ll) + '\n'
    fn, f = backdoor_file(filename)
    f.write(s)
    print filename, 'is', fn
    print s
    return s

def make_script(filename, cmd):
    fn, f = backdoor_file(filename)
    f.write(cmd)
    return fn

dirs = crab_dirs_from_argv()

if not dirs and not options.fake_results:
    sys.exit('space-separated list of existing crab dirs required in argv (see --help)')

if options.dir_sort:
    dirs.sort()
dirs_done = []

#print 'crmon, called with options:'
#pprint(options.__dict__)
#if options.fake_results:
#    print 'using fake results dict from %s' % options.fake_results
#else:
#    print 'checking these %i dirs:' % len(dirs)
#    for d in dirs:
#        print d
ndirs = len(dirs)
print 'crmon: checking %i dir%s, be patient' % (ndirs, 's' if ndirs > 1 else '')

def check_proxies():
    if options.renew_proxies:
        #print 'checking proxies, might ask for password twice (but you can skip it with ^C if you know what you are doing).'
        crab_renew_proxy_if_needed()
        #for d in dirs:
        #    if crab_is_using_server(d):
        #        print_run_cmd('crab -c %s -renewCredential' % d)

check_proxies()

if options.delay:
    print 'delay requested: sleeping %i seconds before doing anything' % options.wait_time
    time.sleep(options.wait_time)

to_resub_done = {}

pass_count = 0
pass_string = ''
last_proxy_check = time.time()

while 1:
    the_time = time.time()
    if (the_time - last_proxy_check)/3600. > 8:
        check_proxies()
        last_proxy_check = the_time

    pass_string = str(int(time.time()))
    if options.continuous:
        print '\n*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*=*\nmaking a pass (#%i string %s):\n' % (pass_count, pass_string)
    if options.fake_results:
        # Fake statuses for debugging the table printing below.
        print 'fake_results dict:'
        results = open(options.fake_results).read().strip()
        #print results
        results = eval(results)
        dirs = sorted(results.keys())
    else:
        # Get the statuses of all the dirs. 
        results = crab_process_statuses_with_redo(dirs, options.max_processes)
        crab_cleanup()
        os.system('mkdir -p /tmp/%s' % os.environ['USER'])
        open('/tmp/%s/crmontmp' % os.environ['USER'], 'wt').write(repr(results))
        #pprint(results)

        # Resubmit those allowed.
        if options.resub:
            to_resub = defaultdict(list)
            for d, res in results.iteritems():
                for job_num in res['jobListByStatus'].get('failed', []):
                    to_resub[d].append(job_num)
# 'Error' not present in job json since 5/14/2017
#                    job = res['jobs'][str(job_num)]
#                    err_code = job['Error'][0]
#                    if any((not options.resub_white_list and not options.resub_black_list,
#                            options.resub_white_list and err_code in options.resub_white_list,
#                            options.resub_black_list and err_code not in options.resub_black_list)):
#                        to_resub[d].append(job_num)

            for d, jobs in to_resub.iteritems():
                jobs = ','.join(str(j) for j in jobs)
                print 'resubmit', d, jobs
                #crab_command('resubmit', dir=d, jobids=jobs, sitewhitelist=options.resub_white_list, siteblacklist=options.resub_black_list, suppress_stdout=False)
                crab_command('resubmit', dir=d, jobids=jobs, suppress_stdout=True)

    # Print task statuses. I don't know what all these mean yet.
    #print
    #print_header('Task statuses:')
    #tasks_by_status = defaultdict(list)
    #for d in dirs:
    #    tasks_by_status[results[d]['status']].append(d)
    #for k,v in tasks_by_status.iteritems():
    #    print_subheader('%s:' % k)
    #    for d in sorted(v):
    #        print d

    if options.verbose:
        # Print a recap of all the status summaries.
        print
        print_header('Recap (everything, sorted by directory):')
        for d in dirs:
            if not results.has_key(d):
                continue
            print_subheader('%s:' % d)
            job_lists = results[d]['jobListByStatus']
            for status in sorted(job_lists.keys()):
                print '%s: %s' % (status.ljust(25), crabify_list(job_lists[status], simple=False))

        print
        print_header('Recap (everything, sorted by status):')
        statuses = defaultdict(list)
        for d in dirs:
            job_lists = results[d]['jobListByStatus']
            for status in job_lists.keys():
                statuses[status].append(d)
        for status in sorted(statuses.keys()):
            print_subheader('%s:' % status)
            these_dirs = [d for d in statuses[status] if d in dirs]
            these_dirs.sort()
            max_w = max(len(d) for d in these_dirs)
            for d in these_dirs:
                job_lists = results[d]['jobListByStatus']
                print '%s %s' % (d.ljust(max_w + 1), crabify_list(job_lists[status], simple=False))

    #print
    #print_header('Recap (skipping exit code 0 but  where X, Y != 0):')
    to_resub = defaultdict(list)
    to_resub.update(to_resub_done)
    #for d in dirs:
    #    statuses = results[d]
    #    printed = False
    #    for status in sorted(statuses.keys()):
    #        if status == options.retrieved_0_str or 'Retrieved' not in status:
    #            continue
    #        if not printed:
    #            print_subheader('%s:' % d)
    #            printed = True
    #        jobs = statuses[status]
    #        if 'Retrieved' in status:
    #            to_resub[d].extend(jobs)
    #        print '%s: %s' % (status.ljust(25), crabify_list(jobs, simple=False))
    #print
    #
    #resub_crlogs = []
    #resub_crrejobs = []
    #resub_crjobprobs = ['cat << EOF | crjobprobs']
    #for d,jobs in to_resub.iteritems():
    #    jobs = crabify_list(jobs, False)
    #    resub_crlogs.append('crlog %s %s ; \\' % (d, jobs))
    #    resub_crrejobs.append('crrejob %s %s doit ; \\' % (d, jobs))
    #    resub_crjobprobs.append('%s %s' % (d, jobs))
    #resub_crjobprobs.append('EOF')
    #
    #if not options.resub_any:
    #    print_header('To view logs for resubmits:')
    #    backdoor_print('crlogs', resub_crlogs)
    #    print_header('To resubmit:')
    #    backdoor_print('crrejobs', resub_crrejobs)
    #print_header('To diagnose by site:')
    #backdoor_print('crjobprobs', resub_crjobprobs)

    # Now print a big summary table where the rows are directories and
    # the columns are the statuses, with the entries being the number
    # of jobs in the directory for each status.

    # Figure out the header row for the status columns. Alphabetical
    # by status, except put finished first. We'll also shorten
    # the status codes, either displaying the short ones all the time,
    # or perhaps only doing it when the table would be wider than the
    # current terminal (not implemented yet).
    status_columns = sorted(set(sum((res['jobsPerStatusEx'].keys() for res in results.itervalues()), [])))
    if 'finished' in status_columns:
        status_columns.remove('finished')
        status_columns = ['finished'] + status_columns

    status_columns_short = []
    status_columns_xlate = {}
    to_replace = [
        ('finished', 'fin'),
        ('failed', 'fail'),
        ('running', 'run'),
        ('trans', 'x'),
        ]

    if options.shorten_statuses:
        for status in status_columns:
            status_short = status
            for a,b in to_replace:
                status_short = status_short.replace(a,b)

            #if '0_0' in status_short:
            #    status_short = status_short.replace('0_0', '0')
            #elif '_' in status_short and status_short.count('_') == 2:
            #    a,b,c = status_short.split('_')
            #    if b == '0':
            #        status_short = status_short.replace('0_', '')
            #    elif b == c:
            #        status_short = a + '_' + b

            status_columns_short.append(status_short)
            status_columns_xlate[status_short] = status

        status_columns = status_columns_short

    status_column_format = ''.join('%' + str(max(len(status_column) + 2, 6)) + 's' for status_column in status_columns)

    # The table format string: a left-justified column for the
    # directory names, followed by columns for all of the statuses
    # seen, with the width found above.
    dir_column_width = max(len(d) for d in results.keys())
    table_format = '%-' + str(dir_column_width) + 's'
    table_format += '%8s' # for the totals column
    table_format += status_column_format
    #print repr(table_format)

    # Header.
    header = ['directory', 'total |'] + status_columns
    print_header(table_format % tuple(header))

    # Print out all the rows. Column order is as above.
    sums = (len(status_columns)+1)*[0]
    for d in dirs:
        statuses = results[d]['jobsPerStatusEx']
        row = []
        nfail, nrun, nfin = 0, 0, 0
        for status in status_columns:
            entry = statuses.get(status_columns_xlate.get(status, status), 0)
            if status.startswith('fin'):
                nfin += entry
            elif status.startswith('fail'):
                nfail += entry
            else:
                nrun += entry
            row.append((status, entry))
        sub_row = tuple(x[1] for x in row)
        row_total = sum(sub_row)
        sums[0] += row_total
        for i, x in enumerate(sub_row):
            sums[i+1] += x
        if options.suppress_zeroes:
            sub_row = tuple('-' if x == 0 else x for x in sub_row)
        row = (d, '%s |' % row_total) + sub_row
        if nfin == row_total:
            color = colors.green
        elif nfail:
            color = colors.magenta
        elif nrun:
            color = colors.cyan
        print color(table_format % row)
    sums[0] = '%s |' % sums[0]
    sums = tuple(['totals'] + sums)
    sum_row = table_format % sums
    print colors.bold(sum_row)
    print

    # Figure out which directories are done, and remove them from the
    # list of dirs to check next time. Also print out a new crmon
    # command for pasting that just resumes with the not-done dirs.

    done_stats = set(''.split())
    done_stats_info_text = repr(tuple(sorted(done_stats)))
    fail = []
    done = []
    notdone = []
    for k,v in results.iteritems():
        if any(stat for stat in v['jobsPerStatusEx'].keys() if 'fail' in stat):
            fail.append(k)
        statuses_left = set(stat for stat in v['jobsPerStatusEx'].keys() if stat != 'finished')
        statuses_left -= done_stats
        (notdone if statuses_left else done).append(k)
    fail.sort(key=dirs.index)
    done.sort(key=dirs.index)
    notdone.sort(key=dirs.index)

    done_msg = '\nThese are done as far as crmon is concerned (all jobs are "finished" or one of %s):' % done_stats_info_text 
    if done:
        if options.verbose:
            print_header(done_msg)
            for d in done:
                print d
            print_header('These are not:')
            for d in notdone:
                print d
            print_header('Stopping monitoring of the former.')
        for d in done:
            if to_resub.has_key(d):
                to_resub_done[d] = to_resub[d]
            dirs.remove(d)
            d_statuses = results[d]['jobsPerStatusEx'].keys()
            if len(d_statuses) != 1 or d_statuses[0] != 'finished':
                dirs_done.append((d,) + tuple(d_statuses))
            else:
                dirs_done.append(d)

    if options.verbose and dirs_done:
        print_header(done_msg)
        for d in dirs_done:
            print d

    if notdone:
        print colors.bold('Resume not-done:')
        print '.', make_script('resume', 'crmon ' + ' '.join(notdone))

    if fail:
        print colors.bold('Resubmit failed:')
        print '.', make_script('resubmit', 'cr res ' + ' '.join(fail))

    if not dirs:
        print_header('All done!')
        break

    if not options.continuous:
        break

    # Might want to break but can't count on being in front of the
    # terminal when a very long iteration finishes. Look for a special
    # file in /tmp/$USER that signals we should break.

    break_fn =  '/tmp/%s/break_crmon' % os.environ['USER']
    if os.path.isfile(break_fn):
        os.remove(break_fn)
        print_header('break file spotted, quitting.')
        break
    
    # Sleep for the specified time.
    
    now = datetime.datetime.now()
    then = now + datetime.timedelta(seconds=options.wait_time)
    print colors.bold('That was pass #%i string %s. Going to sleep for %i seconds at %s. Will wake up at %s.' % (pass_count, pass_string, options.wait_time, now, then))
    print '(Hit Ctrl-C, then enter to immediately start the next iteration. Or, hit Ctrl-C, then Ctrl-C to quit.)'

    pass_count += 1

    try:
        time.sleep(options.wait_time)
    except KeyboardInterrupt:
        raw_input('quit?')
